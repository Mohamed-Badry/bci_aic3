{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b8a2d8",
   "metadata": {},
   "source": [
    "# Training the MI and SSVEP models using a simple CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61913af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382472d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional import (multiclass_f1_score,\n",
    "                                          binary_accuracy,\n",
    "                                          binary_f1_score)\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "# Code necessary to create reproducible runs\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562e65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 threads for computations\n"
     ]
    }
   ],
   "source": [
    "# Set the number of threads for computations\n",
    "torch.set_num_threads(12)\n",
    "print(f\"Using {torch.get_num_threads()} threads for computations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43528c88",
   "metadata": {},
   "source": [
    "### Dataset Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class BCIDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_path, task_type=\"MI\", label_mapping=None):\n",
    "        # Filter the main dataframe for the specific task (MI or SSVEP)\n",
    "        self.metadata = pd.read_csv(os.path.join(base_path, csv_file))\n",
    "        self.metadata = self.metadata[self.metadata[\"task\"] == task_type]\n",
    "        self.base_path = base_path\n",
    "        self.task_type = task_type\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "        # 9 seconds * 250 Hz = 2250 for MI\n",
    "        # 7 seconds * 250 Hz = 1750 for SSVEP\n",
    "        self.sequence_length = 2250 if task_type == \"MI\" else 1750\n",
    "\n",
    "\n",
    "        num_trials = len(self.metadata)\n",
    "\n",
    "        self.tensor_data = torch.empty(\n",
    "            num_trials, self.sequence_length, 8, dtype=torch.float32\n",
    "        )\n",
    "        self.labels = torch.empty(num_trials, dtype=torch.long)\n",
    "\n",
    "        for i, (idx, row) in enumerate(self.metadata.iterrows()):\n",
    "            # Determine dataset split (train/validation/test)\n",
    "            id_num = row[\"id\"]\n",
    "            if id_num <= 4800:\n",
    "                dataset_split = \"train\"\n",
    "            elif id_num <= 4900:\n",
    "                dataset_split = \"validation\"\n",
    "            else:\n",
    "                dataset_split = \"test\"\n",
    "\n",
    "            # Path to the EEG data file\n",
    "            eeg_path = os.path.join(\n",
    "                self.base_path,\n",
    "                row[\"task\"],\n",
    "                dataset_split,\n",
    "                row[\"subject_id\"],\n",
    "                str(row[\"trial_session\"]),\n",
    "                \"EEGdata.csv\",\n",
    "            )\n",
    "\n",
    "            eeg_data = pd.read_csv(eeg_path)\n",
    "\n",
    "            # Extract the correct trial segment\n",
    "            trial_num = int(row[\"trial\"])\n",
    "\n",
    "            samples_per_trial = self.sequence_length\n",
    "            start_idx = (trial_num - 1) * samples_per_trial\n",
    "            end_idx = start_idx + samples_per_trial - 1\n",
    "\n",
    "            # Select only the 8 EEG channels\n",
    "            eeg_channels = [\"FZ\", \"C3\", \"CZ\", \"C4\", \"PZ\", \"PO7\", \"OZ\", \"PO8\"]\n",
    "            trial_data = eeg_data.loc[start_idx : end_idx, eeg_channels].values\n",
    "\n",
    "            # uncomment the line below and comment the one above to include all 18 columns\n",
    "            # trial_data = eeg_data.loc[start_idx:end_idx-1].values\n",
    "\n",
    "            # Preprocess the data (see next section)\n",
    "            processed_data = self.preprocess(trial_data)\n",
    "\n",
    "            # Convert to tensor\n",
    "            tensor_data = torch.tensor(processed_data, dtype=torch.float32)\n",
    "            self.tensor_data[i] = tensor_data\n",
    "\n",
    "            # Get label if it exists\n",
    "            if \"label\" in row and self.label_mapping:\n",
    "                label_str = row[\"label\"]\n",
    "                label_int = self.label_mapping[label_str]\n",
    "                self.labels[i] = label_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels[idx] is not None:\n",
    "            return self.tensor_data[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.tensor_data[idx]\n",
    "\n",
    "    def preprocess(self, eeg_data):\n",
    "        # Apply preprocessing steps here (filtering, normalization, etc.)\n",
    "        # This will be different for MI and SSVEP\n",
    "        # ...\n",
    "        return eeg_data\n",
    "\n",
    "\n",
    "def load_data(base_path, task_type, label_mapping) -> Tuple[BCIDataset, BCIDataset, BCIDataset]:\n",
    "    \"\"\"\n",
    "    Loads the train, val, test data for the given {task_type} inside the given {base_path}\n",
    "\n",
    "    Returns:\n",
    "        a tuple of BCIDataset in the order (train, val, test)\n",
    "    \"\"\"\n",
    "\n",
    "    train = BCIDataset(csv_file=\"train.csv\", base_path=base_path, task_type=task_type, label_mapping=label_mapping)\n",
    "    val = BCIDataset(\n",
    "        csv_file=\"validation.csv\", base_path=base_path, task_type=task_type, label_mapping=label_mapping\n",
    "    )\n",
    "    test = BCIDataset(csv_file=\"test.csv\", base_path=base_path, task_type=task_type, label_mapping=label_mapping)\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493a796",
   "metadata": {},
   "source": [
    "### Simple CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc9de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/simple_cnn.py\n",
    "\n",
    "# Simple CNN model\n",
    "class BCIModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, sequence_length):\n",
    "        super(BCIModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3)  # 8 channels\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Helper to calculate the flattened size\n",
    "        self._get_conv_output_size(input_channels, sequence_length)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, num_classes)\n",
    "        \n",
    "    def _get_conv_output_size(self, input_channels, sequence_length):\n",
    "        # Create a dummy input tensor\n",
    "        dummy_input = torch.randn(1, input_channels, sequence_length)\n",
    "        \n",
    "        # Pass it through the convolutional and pooling layers\n",
    "        output = self.pool(torch.relu(self.conv1(dummy_input)))\n",
    "        \n",
    "        # Calculate the flattened size\n",
    "        self._to_linear = output.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3991b",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7758bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "def rec_cpu_count() -> int:\n",
    "    \"\"\"Returns recommended cpu count based on machine and a simple heuristic\"\"\"\n",
    "    cpu_count = os.cpu_count()\n",
    "\n",
    "    if cpu_count is None:\n",
    "        return 4\n",
    "\n",
    "    return min(cpu_count // 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324de3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bci_aic3.paths import RAW_DATA_DIR\n",
    "\n",
    "# base_path = \"/kaggle/input/mtcaic3\"\n",
    "base_path = RAW_DATA_DIR\n",
    "label_mapping = {'Left': 0, 'Right': 1, 'Forward': 2, 'Backward': 3}\n",
    "task_type = \"MI\"  # MI or SSVEP\n",
    "sequence_length = None\n",
    "\n",
    "if task_type == \"MI\":\n",
    "    num_classes = 2\n",
    "    sequence_length = 2250\n",
    "elif task_type == \"SSVEP\":\n",
    "    num_classes = 4\n",
    "    sequence_length = 1750\n",
    "\n",
    "batch_size = 128\n",
    "max_num_workers = 8\n",
    "\n",
    "# Loading the data\n",
    "train_mi, val_mi, test_ssvep = load_data(\n",
    "    base_path=base_path, task_type=task_type, label_mapping=label_mapping\n",
    ")\n",
    "\n",
    "train_loader_mi = DataLoader(\n",
    "    train_mi, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=max_num_workers,\n",
    ")\n",
    "val_loader_mi = DataLoader(\n",
    "    val_mi, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=max_num_workers, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225be85",
   "metadata": {},
   "source": [
    "## MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37a7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the mi_model, loss, and optimizer\n",
    "mi_model = BCIModel(train_mi[0][0].shape[1], num_classes=num_classes, sequence_length=sequence_length).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mi_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba623b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "with torch.profiler.profile() as prof:\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        mi_model.train()\n",
    "        \n",
    "        for data, labels in train_loader_mi:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = mi_model(data.transpose(1, 2))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        mi_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader_mi:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = mi_model(data.transpose(1, 2))\n",
    "\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        # if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_loader_mi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef3b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_581/4049462476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"self_cpu_time_total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prof' is not defined"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d0588",
   "metadata": {},
   "source": [
    "#### MI Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "403836c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "BCIModel                                 [1, 8, 2250]              [1, 2]                    --\n",
       "├─Conv1d: 1-1                            [1, 8, 2250]              [1, 16, 2248]             400\n",
       "├─MaxPool1d: 1-2                         [1, 16, 2248]             [1, 16, 1124]             --\n",
       "├─Linear: 1-3                            [1, 17984]                [1, 2]                    35,970\n",
       "===================================================================================================================\n",
       "Total params: 36,370\n",
       "Trainable params: 36,370\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.94\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 0.29\n",
       "Params size (MB): 0.15\n",
       "Estimated Total Size (MB): 0.51\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(mi_model, \n",
    "        input_size=(1, 8, 2250),\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ecc23",
   "metadata": {},
   "source": [
    "### MI Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7699049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6111\n",
      "Multiclass F1 Score (micro): 0.4400\n",
      "Multiclass F1 Score (macro): 0.3056\n",
      "Accuracy: 0.4400\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "mi_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader_mi:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        logits = mi_model(x_batch.transpose(1, 2))\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "f1 = binary_f1_score(all_preds, all_labels)\n",
    "print(f\"F1 Score: {f1.item():.4f}\")\n",
    "\n",
    "multi_f1_micro = multiclass_f1_score(all_preds, all_labels, num_classes=2, average='micro')\n",
    "print(f\"Multiclass F1 Score (micro): {multi_f1_micro.item():.4f}\")\n",
    "\n",
    "multi_f1_macro = multiclass_f1_score(all_preds, all_labels, num_classes=2, average='macro')\n",
    "print(f\"Multiclass F1 Score (macro): {multi_f1_macro.item():.4f}\")\n",
    "\n",
    "acc = binary_accuracy(all_preds, all_labels)\n",
    "print(f\"Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5223256",
   "metadata": {},
   "source": [
    "### Save MI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e24bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_save_path = \"mi_model_simple_cnn_cuda.pth\"\n",
    "torch.save({\n",
    "    \"epoch\": epoch,                     \n",
    "    \"model_state_dict\": mi_model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"loss\": loss,\n",
    "    \"f1_score\": multi_f1_macro,\n",
    "}, mi_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800c2bb",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565f6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, input_size, num_classes, sequence_length):\n",
    "    model = BCIModel(input_size, num_classes, sequence_length).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "mi_model_loaded, optimizer_loaded = load_model(\"mi_model_simple_cnn.pth\", 8, 2, 2250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1098a6c",
   "metadata": {},
   "source": [
    "### Loaded Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a7b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass F1 Score (macro): 0.3056\n",
      "Accuracy: 0.4400\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "mi_model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader_mi:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        logits = mi_model_loaded(x_batch.transpose(1, 2))\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "multi_f1_macro = multiclass_f1_score(all_preds, all_labels, num_classes=2, average='macro')\n",
    "print(f\"Multiclass F1 Score (macro): {multi_f1_macro.item():.4f}\")\n",
    "\n",
    "acc = binary_accuracy(all_preds, all_labels)\n",
    "print(f\"Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b333f7",
   "metadata": {},
   "source": [
    "## SSVEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2db7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/kaggle/input/mtcaic3\"\n",
    "# base_path = \"/data/raw/mtcaic3\"\n",
    "label_mapping = {'Left': 0, 'Right': 1, 'Forward': 2, 'Backward': 3}\n",
    "task_type = \"SSVEP\"  # MI or SSVEP\n",
    "sequence_length = None\n",
    "\n",
    "if task_type == \"MI\":\n",
    "    num_classes = 2\n",
    "    sequence_length = 2250\n",
    "elif task_type == \"SSVEP\":\n",
    "    num_classes = 4\n",
    "    sequence_length = 1750\n",
    "\n",
    "batch_size = 128\n",
    "max_num_workers = rec_cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e7f50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the data\n",
    "train_ssvep, val_ssvep, test_ssvep = load_data(\n",
    "    base_path=base_path, task_type=task_type, label_mapping=label_mapping\n",
    ")\n",
    "\n",
    "train_loader_ssvep = DataLoader(\n",
    "    train_ssvep, batch_size=batch_size, shuffle=True, num_workers=max_num_workers\n",
    ")\n",
    "val_loader_ssvep = DataLoader(\n",
    "    val_ssvep, batch_size=batch_size, shuffle=False, num_workers=max_num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec9a8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EEGNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92da95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "\n",
    "class EEGNet_SSVEP(nn.Module):\n",
    "    \"\"\"SSVEP Variant of EEGNet, as used in [1].\n",
    "    \n",
    "    Inputs:\n",
    "        nb_classes      : int, number of classes to classify\n",
    "        Chans, Samples  : number of channels and time points in the EEG data\n",
    "        dropoutRate     : dropout fraction\n",
    "        kernLength      : length of temporal convolution in first layer\n",
    "        F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                          filters (F2) to learn.\n",
    "        D               : number of spatial filters to learn within each temporal\n",
    "                          convolution.\n",
    "        dropoutType     : Either 'SpatialDropout2D' or 'Dropout', passed as a string.\n",
    "    \n",
    "    [1]. Waytowich, N. et. al. (2018). Compact Convolutional Neural Networks\n",
    "    for Classification of Asynchronous Steady-State Visual Evoked Potentials.\n",
    "    Journal of Neural Engineering vol. 15(6).\n",
    "    http://iopscience.iop.org/article/10.1088/1741-2552/aae5d8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_classes=4, Chans=8, Samples=1750, \n",
    "                 dropoutRate=0.25, kernLength=250, F1=96, \n",
    "                 D=1, F2=96, dropoutType='Dropout'):\n",
    "        super(EEGNet_SSVEP, self).__init__()\n",
    "        \n",
    "        self.nb_classes = nb_classes\n",
    "        self.Chans = Chans\n",
    "        self.Samples = Samples\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.kernLength = kernLength\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F2\n",
    "        \n",
    "        # Validate dropout type\n",
    "        if dropoutType not in ['SpatialDropout2D', 'Dropout']:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                           'or Dropout, passed as a string.')\n",
    "        self.dropoutType = dropoutType\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        # Depthwise convolution - equivalent to DepthwiseConv2D\n",
    "        self.depthwise_conv = nn.Conv2d(F1, F1 * D, (Chans, 1), \n",
    "                                       groups=F1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(F1 * D)\n",
    "        \n",
    "        # Apply max_norm constraint to depthwise conv weights\n",
    "        self.depthwise_conv = weight_norm(self.depthwise_conv, name='weight')\n",
    "        \n",
    "        self.avgpool1 = nn.AvgPool2d((1, 4))\n",
    "        \n",
    "        # Block 2\n",
    "        # SeparableConv2D is equivalent to depthwise + pointwise convolution\n",
    "        self.separable_conv_depthwise = nn.Conv2d(F1 * D, F1 * D, (1, 16), \n",
    "                                                 groups=F1 * D, padding='same', bias=False)\n",
    "        self.separable_conv_pointwise = nn.Conv2d(F1 * D, F2, 1, bias=False)\n",
    "        \n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2)\n",
    "        self.avgpool2 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        # Calculate the size after convolutions for the linear layer\n",
    "        # We need to be more careful about the feature size calculation\n",
    "        # This will be computed dynamically in the first forward pass\n",
    "        self.feature_size = None\n",
    "        \n",
    "        # Dense layer - will be initialized on first forward pass\n",
    "        self.classifier = None\n",
    "        \n",
    "        # Dropout layers\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout1 = SpatialDropout2d(dropoutRate)\n",
    "            self.dropout2 = SpatialDropout2d(dropoutRate)\n",
    "        else:  # 'Dropout'\n",
    "            self.dropout1 = nn.Dropout2d(dropoutRate)\n",
    "            self.dropout2 = nn.Dropout2d(dropoutRate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights similar to Keras defaults\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle different input formats\n",
    "        if x.dim() == 3:\n",
    "            # Input shape: (batch_size, samples, channels)\n",
    "            # Reshape to (batch_size, channels, samples, 1) then to (batch_size, 1, channels, samples)\n",
    "            x = x.permute(0, 2, 1)  # (batch_size, channels, samples)\n",
    "            x = x.unsqueeze(1)      # (batch_size, 1, channels, samples)\n",
    "        elif x.dim() == 4:\n",
    "            # Input shape: (batch_size, Chans, Samples, 1)\n",
    "            # Reshape to (batch_size, 1, Chans, Samples) for PyTorch conv2d\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected input to be 3D or 4D, got {x.dim()}D tensor\")\n",
    "        \n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.separable_conv_depthwise(x)\n",
    "        x = self.separable_conv_pointwise(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Check if we can apply the second pooling\n",
    "        if x.size(-1) >= 8:  # Check if temporal dimension is >= 8\n",
    "            x = self.avgpool2(x)\n",
    "        else:\n",
    "            # Use adaptive pooling or smaller kernel\n",
    "            x = F.avg_pool2d(x, (1, min(x.size(-1), 2)))\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Initialize classifier on first forward pass\n",
    "        if self.classifier is None:\n",
    "            self.feature_size = x.size(1)\n",
    "            self.classifier = nn.Linear(self.feature_size, self.nb_classes).to(x.device)\n",
    "            # Initialize the classifier weights\n",
    "            nn.init.kaiming_normal_(self.classifier.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(self.classifier.bias, 0)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # Softmax (often applied in loss function, but included here for completeness)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialDropout2d(nn.Module):\n",
    "    \"\"\"Spatial Dropout implementation for PyTorch\n",
    "    \n",
    "    Drops entire feature maps instead of individual elements.\n",
    "    This is equivalent to Keras' SpatialDropout2D.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super(SpatialDropout2d, self).__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        \n",
    "        # x shape: (N, C, H, W)\n",
    "        N, C, H, W = x.size()\n",
    "        \n",
    "        # Create mask for entire feature maps\n",
    "        mask = torch.bernoulli(torch.full((N, C, 1, 1), 1 - self.p, device=x.device))\n",
    "        mask = mask.expand_as(x)\n",
    "        \n",
    "        # Apply mask and scale\n",
    "        return x * mask / (1 - self.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53785a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model, loss, and optimizer\n",
    "# ssvep_model = BCIModel(train_ssvep[0][0].shape[1], num_classes=num_classes, sequence_length=sequence_length).to(device)\n",
    "ssvep_model = EEGNet_SSVEP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ssvep_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "34c35a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:34<10:49,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val Loss: 1.4636753797531128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:08<10:09,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val Loss: 1.463667869567871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [01:42<09:38,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Val Loss: 1.4636681079864502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:16<09:03,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Val Loss: 1.48002290725708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [02:49<08:28,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Val Loss: 1.429288625717163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [03:23<07:54,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Val Loss: 1.4636681079864502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [03:57<07:21,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Val Loss: 1.463456392288208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:31<06:47,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Val Loss: 1.5036680698394775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [05:05<06:14,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Val Loss: 1.560676097869873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [05:39<05:39,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [06:13<05:05,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [06:47<04:31,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [07:21<03:57,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [07:55<03:23,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [08:29<02:49,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Val Loss: 1.4836680889129639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [09:03<02:16,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Val Loss: 1.463484764099121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [09:37<01:41,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [10:11<01:07,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [10:45<00:33,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Val Loss: 1.4636681079864502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:19<00:00,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Val Loss: 1.5436680316925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    ssvep_model.train()\n",
    "    for data, labels in train_loader_ssvep:\n",
    "\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssvep_model(data.transpose(1, 2))\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    ssvep_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader_ssvep:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = ssvep_model(data.transpose(1, 2))\n",
    "\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_loader_ssvep)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db572d",
   "metadata": {},
   "source": [
    "#### SSVEP Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "043e3c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "EEGNet_SSVEP                             [128, 8, 1750]            [128, 4]                  --\n",
       "├─Conv2d: 1-1                            [128, 1, 1750, 8]         [128, 96, 1750, 8]        24,000\n",
       "├─BatchNorm2d: 1-2                       [128, 96, 1750, 8]        [128, 96, 1750, 8]        192\n",
       "├─ParametrizedConv2d: 1-3                [128, 96, 1750, 8]        [128, 96, 1743, 8]        --\n",
       "│    └─ModuleDict: 2-1                   --                        --                        --\n",
       "│    │    └─ParametrizationList: 3-1     --                        [96, 1, 8, 1]             769\n",
       "├─BatchNorm2d: 1-4                       [128, 96, 1743, 8]        [128, 96, 1743, 8]        192\n",
       "├─AvgPool2d: 1-5                         [128, 96, 1743, 8]        [128, 96, 1743, 2]        --\n",
       "├─Dropout2d: 1-6                         [128, 96, 1743, 2]        [128, 96, 1743, 2]        --\n",
       "├─Conv2d: 1-7                            [128, 96, 1743, 2]        [128, 96, 1743, 2]        1,536\n",
       "├─Conv2d: 1-8                            [128, 96, 1743, 2]        [128, 96, 1743, 2]        9,216\n",
       "├─BatchNorm2d: 1-9                       [128, 96, 1743, 2]        [128, 96, 1743, 2]        192\n",
       "├─Dropout2d: 1-10                        [128, 96, 1743, 1]        [128, 96, 1743, 1]        --\n",
       "├─Linear: 1-11                           [128, 167328]             [128, 4]                  669,316\n",
       "===================================================================================================================\n",
       "Total params: 705,413\n",
       "Trainable params: 705,413\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 47.89\n",
       "===================================================================================================================\n",
       "Input size (MB): 7.17\n",
       "Forward/backward pass size (MB): 5151.33\n",
       "Params size (MB): 2.82\n",
       "Estimated Total Size (MB): 5161.32\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(ssvep_model, \n",
    "        input_size=(128, 8, 1750),\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40aa849",
   "metadata": {},
   "source": [
    "### SSVEP Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1e934ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c1ad3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass F1 Score (micro): 0.2000\n",
      "Multiclass F1 Score (macro): 0.1264\n",
      "Multiclass Accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "ssvep_model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader_ssvep:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        logits = ssvep_model(x_batch.transpose(1, 2))\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "multi_f1_micro = multiclass_f1_score(all_preds, all_labels, num_classes=num_classes, average='micro')\n",
    "print(f\"Multiclass F1 Score (micro): {multi_f1_micro.item():.4f}\")\n",
    "\n",
    "multi_f1_macro = multiclass_f1_score(all_preds, all_labels, num_classes=num_classes, average='macro')\n",
    "print(f\"Multiclass F1 Score (macro): {multi_f1_macro.item():.4f}\")\n",
    "\n",
    "acc = multiclass_accuracy(all_preds, all_labels, num_classes=num_classes)\n",
    "print(f\"Multiclass Accuracy: {acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c7355d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 2, 2, 2, 0, 1, 3, 0, 2, 3, 1, 1, 0, 2, 0, 1, 2, 0, 3, 2, 0, 3, 1,\n",
       "        0, 2, 2, 3, 0, 1, 1, 3, 3, 3, 0, 3, 1, 1, 2, 3, 0, 0, 2, 0, 3, 1, 0, 2,\n",
       "        3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66e0c7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d65d76",
   "metadata": {},
   "source": [
    "### SSVEP Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48c48b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvep_save_path = \"ssvep_model_simple_cnn.pth\"\n",
    "torch.save({\n",
    "    \"epoch\": epoch,                     \n",
    "    \"model_state_dict\": ssvep_model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"loss\": loss,\n",
    "    \"f1_score\": multi_f1_macro,\n",
    "}, ssvep_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29271e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
